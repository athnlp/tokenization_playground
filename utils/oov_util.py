import json

from vocab import TokenizerImpl, all_tokenizer_config, load_tokenizer

text = (
    "hello; Ğ—Ğ°Ğ¼Ğ³Ğ»Ğ°Ğ²Ñ‹ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ; íŠ¹íˆ ì£¼ì†Œ 15~17ë²ˆ í™€ì—ì„  3ì—°ì†;"
    " ç¢ºå®Ÿã«æ˜¥ãŒè¿‘ã¥ã„ã¦ã‚‹ã“ã¨;  a kÃ¶zoktatÃ¡ssal? _ BelfÃ¶ld;"
    " pumÃ«, i vjetÃ«r, vjeÃ§; Ø¦Û•Ø±Ø¯ÙˆØºØ§Ù† Û‹Û• Ù‚Ù‰Ø±ØºÙ‰Ø²Ù‰Ø³ØªØ§Ù† ;"
    " à¤¨à¤¿à¤®à¥à¤¨ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤•à¥Œà¤¨ à¤¸à¤¾ à¤¹à¤¾à¤°à¤¡à¤µà¥‡à¤¯à¤°; á‰°áˆˆá‹‹á‹‹áŒ­ á‹¨áŒá‹µáŒá‹³ ; Ğ”Ğ·ĞµĞ¹Ğ½Ñ‹Ñ Ğ°ÑĞ¾Ğ±Ñ‹:;"
    " Â« àª…àª®àª°à«‡àª²à«€àª¨àª¾àª‚ àª®àª¹àª¿àª²àª¾ àªµàª¿àª•àª¾àª¸; ğŸ¦™â¤â¥ì›ƒìœ â™‹â˜®âœŠ;"
    "×°×™×§×™×°×¢×¨×˜×¢×¨×‘×•×š "
)
whitespace = "\t   \n\n\r  "
bytes = b"\x00\x01\x02\x03\x04".decode("utf-8")

text += whitespace


def get_unk(tokenizer_config):
    tokenizer = load_tokenizer(tokenizer_config)
    if hasattr(tokenizer, "unk_token"):
        return f"{tokenizer.unk_token}, {tokenizer.unk_token_id}"
    else:
        return "unk_token not found"


# def infer_tokenizer_impl(tokenizer_config):
def infer_tokenizer_type(tokenizer_config):
    tokenizer = load_tokenizer(tokenizer_config)
    if tokenizer_config.impl == TokenizerImpl.TikToken:
        return "tiktoken"
    if hasattr(tokenizer, "backend_tokenizer"):
        return str(
            type(tokenizer.backend_tokenizer.model)
        )  # type(tokenizer._tokenizer.model))
    # orion: sp_model.Load(vocab_file)ï¼Œç»§æ‰¿ PreTrainedTokenizer
    elif hasattr(tokenizer, "sp_model"):  # åŸºäº sentencepiece åŒ…
        # for i in range(tokenizer.sp_model.piece_size()):
        #     if tokenizer.sp_model.is_byte(i):
        #         print("")
        return f"sp_model, byte_num: {sum([tokenizer.sp_model.is_byte(i) for i in range(tokenizer.sp_model.piece_size())])}"

    # sp.Load(model_path)  ï¼Œå¹¶ä¸”åŒ…æ‹¬image_tokenizer
    elif "glm-" in tokenizer_config.name_or_path:
        return f"byte_num: {sum([tokenizer.sp_tokenizer.text_tokenizer.sp.is_byte(i) for i in range(tokenizer.sp_tokenizer.text_tokenizer.sp.piece_size())])}"
    # sp.Load(model_path)  ï¼Œæ²¡æœ‰image_tokenizer
    elif (
        "glm2-" in tokenizer_config.name_or_path
        or "glm3-" in tokenizer_config.name_or_path
        or "CharacterGLM-6B" in tokenizer_config.name_or_path
    ):
        return f"byte_num: {sum([tokenizer.tokenizer.sp_model.is_byte(i) for i in range(tokenizer.tokenizer.sp_model.piece_size())])}"
    elif (
        "abeja/gpt-neox-japanese-2.7b" == tokenizer_config.name_or_path
    ):  # æ”¯æŒ byte-levelï¼Œè§£å†³oové—®é¢˜
        return "japanese-bpe: https://github.com/tanreinama/Japanese-BPEEncoder_V2"
    # bert-base-japaneseï¼š ç‰¹æ®Šçš„åœ°æ–¹åœ¨äº "word_tokenizer_type": "mecab"ï¼Œè§ https://huggingface.co/tohoku-nlp/bert-base-japanese/blob/main/tokenizer_config.json
    elif "bert-base-japanese" in tokenizer_config.name_or_path:
        return (
            "wordpiece.MecabTokenizer, æ”¯æŒbyte-level https://taku910.github.io/mecab/"
        )
    elif "moss" in tokenizer_config.name_or_path:
        return "åº”è¯¥æ˜¯ sentencepiece.byte_bpe,å¾…ç¡®è®¤"
    elif "byt5" in tokenizer_config.name_or_path:
        return "æœªçŸ¥ï¼Œå¾…å®š"
    else:
        print("catch", tokenizer_config.name_or_path)
        raise "error"


def test_lossless(tokenizer_config):
    """
    xlm-roberta-base ä¸ºä»€ä¹ˆoovè¿™ä¹ˆå°‘ï¼Ÿæ˜¯å› ä¸ºæœ‰ byteå—ï¼Ÿ
    :param tokenizer_config:
    :return:
    """
    tokenizer = load_tokenizer(tokenizer_config)
    encoding = tokenizer.encode(text, add_special_tokens=False)
    decoding = tokenizer.decode(encoding)

    if text in decoding:
        # print(tokenizer_config.name, tokenizer_config.impl, "lossless: true")
        pass
    else:
        unk_count = sum(
            [1 for token_id in encoding if token_id == tokenizer.unk_token_id]
        )
        oov_tokens = []
        # if tokenizer_config.impl == TokenizerImpl.SentencePiece:
        #     print(sum([tokenizer.is_byte(i) for i in range(tokenizer.piece_size())]))

        print("#######" * 5)
        print(
            f"{tokenizer_config.name_or_path}, {infer_tokenizer_type(tokenizer_config)}\n"
            f"lossless: false; unk_token: {get_unk(tokenizer_config)},"
            f" unk_ratio: {unk_count/len(encoding):.4f}; oov: []"
        )
        for i in range(len(text)):
            if text[i] != decoding[i]:
                # print(f"text[{i}]     = {str(bytes(text[i:], 'utf-8'))}\n"
                #       f"decoding[{i}] = {str(bytes(decoding[i:], 'utf-8'))}")
                print(
                    f"text[{i}]     = {json.dumps(text[i:], ensure_ascii=False)}, \n"
                    f"decoding[{i}] = {json.dumps(decoding[i:], ensure_ascii=False)}"
                )

                break


for config in all_tokenizer_config:
    # if "xlm-roberta-base" in config.name:
    # if "xlm-roberta-base" in config.name:
    # if "chatglm3-6b" in config.name:
    # if "bert-base-japanese" in config.name:
    # if "moss" in config.name:
    # if "byt5" in config.name:
    if "baichuan" in config.name_or_path:
        # if "CharacterGLM-6B" in config.name:
        # if "fastchat-t5" in config.name:  # æŠ¥é”™ pyo3_runtime.PanicException: AddedVocabulary bad split
        # if True:
        # test_unk(config)
        test_lossless(config)
